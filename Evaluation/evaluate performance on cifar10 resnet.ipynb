{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1154ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from captum.attr import *\n",
    "import quantus\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9015c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1d31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b285e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rev2.cifar10.model_utils import resnet50, CIFAR10_RESNET50_CKPT_PATH\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, out_keys=None):\n",
    "        out = {}\n",
    "        x = self.conv1(x)\n",
    "        out[\"c1\"] = x\n",
    "        x = self.bn1(x)\n",
    "        out[\"bn1\"] = x\n",
    "        x = F.relu(x)\n",
    "        out[\"r1\"] = x\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        out[\"l1\"] = x\n",
    "        x = self.layer2(x)\n",
    "        out[\"l2\"] = x\n",
    "        x = self.layer3(x)\n",
    "        out[\"l3\"] = x\n",
    "        x = self.layer4(x)\n",
    "        out[\"l4\"] = x\n",
    "\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        out[\"gvp\"] = x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        out[\"fc\"] = x\n",
    "\n",
    "        if out_keys is None:\n",
    "            return x\n",
    "        res = {}\n",
    "        for key in out_keys:\n",
    "            res[key] = out[key]\n",
    "        return res\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1,3,32,32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d45233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar_model(path):\n",
    "    model = resnet50()\n",
    "    ckpt_dict = torch.load(path, lambda storage, loc: storage)\n",
    "    model.load_state_dict(ckpt_dict)\n",
    "    model.to('cuda')\n",
    "    model.train(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd1f181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"/data/virtual environments/adv detection by robustness/adv_detection/Adaptive attacks/Models/CIFAR10/resnet50/cifar.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc077361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_model = load_cifar_model(modelpath)\n",
    "normal_model.to(device)\n",
    "normal_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ef0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_loader_cifar = DataLoader(trainset, shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854473a4",
   "metadata": {},
   "source": [
    "# For given adversarial images and benign images, collect metrics of feature attribution sensitivity and model prediction sensitivity. Save in csv that will be used for inspecting detection performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4cc932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d271c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x_batch, y_batch, spread):\n",
    "    new_x_batch = []\n",
    "    for x in x_batch:\n",
    "        x = x.data.cpu().numpy()\n",
    "        stdev = spread * (np.max(x)-np.min(x))\n",
    "        noise = np.random.normal(0, stdev, x.shape).astype(np.float32)\n",
    "        x_plus_noise = x + noise\n",
    "        x_plus_noise = np.clip(x_plus_noise, 0, 1)\n",
    "        x_plus_noise = torch.from_numpy(x_plus_noise).cpu()\n",
    "        new_x_batch.append(x_plus_noise)\n",
    "    new_batch = torch.stack(new_x_batch).to(device)\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a21aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_benign(adv_path, normal_model): \n",
    "    \n",
    "    print(\"Computing metrics for {} for benign\")\n",
    "    \n",
    "    npobj = np.load(adv_path)\n",
    "    adaptive_image = npobj['b_images']\n",
    "    adaptive_label = npobj['b_labels']\n",
    "    \n",
    "    \n",
    "    #attribution robustness\n",
    "    attribution_gaussian1 = []\n",
    "    attribution_gaussian2 = []\n",
    "    attribution_gaussian3 = []\n",
    "    \n",
    "    #logit robustness\n",
    "    logit_gaussian1 = []\n",
    "    logit_gaussian2 = []\n",
    "    logit_gaussian3 = []\n",
    "    \n",
    "    images, labels = torch.from_numpy(adaptive_image), torch.from_numpy(adaptive_label)\n",
    "    #images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    end = len(adaptive_label)\n",
    "    if end > 1000:\n",
    "        end = 1000\n",
    "    \n",
    "    for i in range(0, end, 2):\n",
    "        \n",
    "        images_adv, y_pred_adv = images[i:i+2], labels[i:i+2]\n",
    "        images_adv, y_pred_adv = images_adv.to(device), y_pred_adv.to(device)\n",
    "        \n",
    "        x_logits = normal_model(images_adv)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #approach: attribution and logit robustness\n",
    "        a_batch = quantus.explain(\n",
    "            model=normal_model, inputs=images_adv, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        gaussian_noisy_images_1 = make_noise(images_adv, y_pred_adv, spread = 0.05)\n",
    "        gaussian_logits_1 = normal_model(gaussian_noisy_images_1)\n",
    "        gaussian_noisy_images_2 = make_noise(images_adv, y_pred_adv, spread = 0.10)\n",
    "        gaussian_logits_2 = normal_model(gaussian_noisy_images_2)\n",
    "        gaussian_noisy_images_3 = make_noise(images_adv, y_pred_adv, spread = 0.15)\n",
    "        gaussian_logits_3 = normal_model(gaussian_noisy_images_3)\n",
    "        \n",
    "        diff1 = torch.norm(x_logits-gaussian_logits_1,p=1, dim=1) \n",
    "        diff2 = torch.norm(x_logits-gaussian_logits_2,p=1, dim=1) \n",
    "        diff3 = torch.norm(x_logits-gaussian_logits_3,p=1, dim=1) \n",
    "        \n",
    "        logit_gaussian1.extend(diff1.detach().cpu().numpy())\n",
    "        logit_gaussian2.extend(diff2.detach().cpu().numpy())\n",
    "        logit_gaussian3.extend(diff3.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        a_batch_gaussian1 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_1, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian2 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_2, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian3 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_3, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian1):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian1.append(c)\n",
    "            \n",
    "        for a, b in zip(a_batch, a_batch_gaussian2):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian2.append(c)\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian3):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian3.append(c)\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame([\n",
    "            \n",
    "            attribution_gaussian1,\n",
    "            attribution_gaussian2,\n",
    "            attribution_gaussian3,\n",
    "            logit_gaussian1,\n",
    "            logit_gaussian2,\n",
    "            logit_gaussian3], index = [\n",
    "            \"Gaussian1 attribution\", \n",
    "            \"Gaussian2 attribution\", \n",
    "            \"Gaussian3 attribution\", \n",
    "            \"Gaussian1 logit robusntess\",\n",
    "            \"Gaussian2 logit robusntess\",\n",
    "            \"Gaussian3 logit robusntess\",\n",
    "                    ])\n",
    "            \n",
    "    path = \"adaptive_Benign.csv\"\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77980925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_adv(adv_path, normal_model): \n",
    "    \n",
    "    print(\"Computing metrics for {} for adv\")\n",
    "    npobj = np.load(adv_path)\n",
    "    adaptive_image = npobj['a_images']\n",
    "    adaptive_label = npobj['a_labels']\n",
    "    \n",
    "    #attribution robustness\n",
    "    attribution_gaussian1 = []\n",
    "    attribution_gaussian2 = []\n",
    "    attribution_gaussian3 = []\n",
    "    \n",
    "    #logit robustness\n",
    "    logit_gaussian1 = []\n",
    "    logit_gaussian2 = []\n",
    "    logit_gaussian3 = []\n",
    "    \n",
    "    images, labels = torch.from_numpy(adaptive_image), torch.from_numpy(adaptive_label)\n",
    "    #images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    end = len(adaptive_label)\n",
    "    if end > 1000:\n",
    "        end = 1000\n",
    "    \n",
    "    for i in range(0, end, 2):\n",
    "        \n",
    "        images_adv, y_pred_adv = images[i:i+2], labels[i:i+2]\n",
    "        images_adv, y_pred_adv = images_adv.to(device), y_pred_adv.to(device)\n",
    "        \n",
    "        x_logits = normal_model(images_adv)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #approach: attribution and logit robustness\n",
    "        a_batch = quantus.explain(\n",
    "            model=normal_model, inputs=images_adv, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        gaussian_noisy_images_1 = make_noise(images_adv, y_pred_adv, spread = 0.05)\n",
    "        gaussian_logits_1 = normal_model(gaussian_noisy_images_1)\n",
    "        gaussian_noisy_images_2 = make_noise(images_adv, y_pred_adv, spread = 0.10)\n",
    "        gaussian_logits_2 = normal_model(gaussian_noisy_images_2)\n",
    "        gaussian_noisy_images_3 = make_noise(images_adv, y_pred_adv, spread = 0.15)\n",
    "        gaussian_logits_3 = normal_model(gaussian_noisy_images_3)\n",
    "        \n",
    "        diff1 = torch.norm(x_logits-gaussian_logits_1,p=1, dim=1) \n",
    "        diff2 = torch.norm(x_logits-gaussian_logits_2,p=1, dim=1) \n",
    "        diff3 = torch.norm(x_logits-gaussian_logits_3,p=1, dim=1) \n",
    "        \n",
    "        logit_gaussian1.extend(diff1.detach().cpu().numpy())\n",
    "        logit_gaussian2.extend(diff2.detach().cpu().numpy())\n",
    "        logit_gaussian3.extend(diff3.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        a_batch_gaussian1 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_1, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian2 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_2, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian3 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_3, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian1):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian1.append(c)\n",
    "            \n",
    "        for a, b in zip(a_batch, a_batch_gaussian2):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian2.append(c)\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian3):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian3.append(c)\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame([\n",
    "            \n",
    "            attribution_gaussian1,\n",
    "            attribution_gaussian2,\n",
    "            attribution_gaussian3,\n",
    "            logit_gaussian1,\n",
    "            logit_gaussian2,\n",
    "            logit_gaussian3], index = [\n",
    "            \"Gaussian1 attribution\", \n",
    "            \"Gaussian2 attribution\", \n",
    "            \"Gaussian3 attribution\", \n",
    "            \"Gaussian1 logit robusntess\",\n",
    "            \"Gaussian2 logit robusntess\",\n",
    "            \"Gaussian3 logit robusntess\",\n",
    "                    ])\n",
    "            \n",
    "    path = \"adaptive_Adv.csv\"\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4c3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TPR(adv1, a, b, adv2, c, d): \n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP = 0 \n",
    "    TP=0\n",
    "    \n",
    "    for value1, value2 in zip(adv1, adv2): \n",
    "        if value1<a or value1>b:\n",
    "            TP += 1\n",
    "        else:\n",
    "            if value2<c or value2>d:\n",
    "                TP+=1\n",
    "            else: \n",
    "                FN+=1\n",
    "    \n",
    "    return (TP/(TP+FN))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed11de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FPR(ap2a, k, l, ap2b, m, n): \n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0 \n",
    "    TP=0\n",
    "    \n",
    "    for value6, value7 in zip(ap2a,ap2b):\n",
    "        if value6<k or value6>l:\n",
    "            FP +=1\n",
    "        else:\n",
    "            if value7<m or value7>n:\n",
    "                FP +=1\n",
    "\n",
    "    return (FP/(len(ap2a)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfc203be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6b6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_auc(adv_path, model):\n",
    "    #logitgaussian3\n",
    "\n",
    "\n",
    "    k=[28,54, 62,76, 86, 96, 106, 126, 150, 200, 400]\n",
    "    l=[340,340,340,340,340,340,340,340,340,340,340]\n",
    "\n",
    "    #attrgaussian3\n",
    "    m=[1800,2200,2700,3100, 3500, 4000, 4500, 5000, 5500, 6000, 8000]\n",
    "    n=[8500,8500, 8500,8500,8500,8500,8500,8500,8500,8500,8500]\n",
    "\n",
    "    \n",
    "    compute_metrics_benign(adv_path, model)\n",
    "    compute_metrics_adv(adv_path, model)\n",
    "    df_cifar = pd.read_csv(\"adaptive_Benign.csv\")\n",
    "    attr_gaussian3 = df_cifar.iloc[2].values.flatten().tolist()[1:]\n",
    "    logit_gaussian3 = df_cifar.iloc[5].values.flatten().tolist()[1:]\n",
    "        \n",
    "    fpr_results =[]\n",
    "    for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "        FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "        fpr_results.append(FPR/100)\n",
    "        \n",
    "    df_pgd_eps1 = pd.read_csv(\"adaptive_Adv.csv\")\n",
    "    attr_gaussian3_eps1 = df_pgd_eps1.iloc[2].values.flatten().tolist()[1:]\n",
    "    logit_gaussian3_eps1 = df_pgd_eps1.iloc[5].values.flatten().tolist()[1:]\n",
    "    \n",
    "    tpr_results =[]\n",
    "    for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "        TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "        tpr_results.append(TPR/100)\n",
    "    return(sklearn.metrics.auc(fpr_results, tpr_results), fpr_results, tpr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aec736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM\n",
    "adv_path1 = 'adv samples/CIFAR/FGSM/0.03137254901960784eps.npz'\n",
    "adv_path2 = 'adv samples/CIFAR/FGSM/0.06274509803921569eps.npz'\n",
    "adv_path3 = 'adv samples/CIFAR/FGSM/0.12549019607843137eps.npz'\n",
    "adv_path4 = 'adv samples/CIFAR/FGSM/0.25098039215686274eps.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a37e03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.87553,\n",
       " [0.021, 0.051, 0.096, 0.206, 0.348, 0.549, 0.709, 0.875, 0.965, 0.996, 1.0],\n",
       " [0.016,\n",
       "  0.21899999999999997,\n",
       "  0.58,\n",
       "  0.855,\n",
       "  0.955,\n",
       "  0.996,\n",
       "  0.9990000000000001,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc, tpr, fpr = return_auc(adv_path1, normal_model)\n",
    "auc, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c20c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.975131, [0.018, 0.05, 0.096, 0.209, 0.355, 0.553, 0.704, 0.884, 0.964, 0.995, 1.0], [0.644, 0.9739999999999999, 0.998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "----\n",
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.988, [0.012, 0.04100000000000001, 0.083, 0.21, 0.331, 0.52, 0.703, 0.8690000000000001, 0.963, 0.996, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "----\n",
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.98, [0.02, 0.051, 0.103, 0.214, 0.3390000000000001, 0.534, 0.706, 0.861, 0.963, 0.9940000000000001, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "print(return_auc(adv_path2, normal_model))\n",
    "print('----')\n",
    "print(return_auc(adv_path3, normal_model))\n",
    "print('----')\n",
    "print(return_auc(adv_path4, normal_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04ab3c",
   "metadata": {},
   "source": [
    "# PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1c116ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_path1 = 'adv samples/CIFAR/PGD/0.03137254901960784eps.npz'\n",
    "adv_path2 = 'adv samples/CIFAR/PGD/0.06274509803921569eps.npz'\n",
    "adv_path3 = 'adv samples/CIFAR/PGD/0.12549019607843137eps.npz'\n",
    "adv_path4 = 'adv samples/CIFAR/PGD/0.25098039215686274eps.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f6ccd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.847106,\n",
       " [0.013000000000000001,\n",
       "  0.048,\n",
       "  0.10100000000000002,\n",
       "  0.203,\n",
       "  0.32899999999999996,\n",
       "  0.515,\n",
       "  0.6830000000000002,\n",
       "  0.8539999999999999,\n",
       "  0.954,\n",
       "  0.99,\n",
       "  1.0],\n",
       " [0.131,\n",
       "  0.269,\n",
       "  0.565,\n",
       "  0.748,\n",
       "  0.885,\n",
       "  0.954,\n",
       "  0.98,\n",
       "  0.996,\n",
       "  0.9990000000000001,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc, tpr, fpr = return_auc(adv_path1, normal_model)\n",
    "auc, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb794b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.923804, [0.029000000000000005, 0.055, 0.111, 0.225, 0.368, 0.552, 0.7289999999999999, 0.903, 0.978, 0.996, 1.0], [0.368, 0.596, 0.857, 0.9469999999999998, 0.98, 0.9990000000000001, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "----\n",
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.965324, [0.029000000000000005, 0.05, 0.109, 0.21099999999999997, 0.348, 0.531, 0.709, 0.882, 0.966, 0.992, 1.0], [0.6919999999999998, 0.9469999999999998, 0.996, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "----\n",
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.9799650000000001, [0.02, 0.055, 0.106, 0.22, 0.337, 0.537, 0.711, 0.883, 0.966, 0.997, 1.0], [0.998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "print(return_auc(adv_path2, normal_model))\n",
    "print('----')\n",
    "print(return_auc(adv_path3, normal_model))\n",
    "print('----')\n",
    "print(return_auc(adv_path4, normal_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759050a",
   "metadata": {},
   "source": [
    "# BIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9cb26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_path1 = 'adv samples/CIFAR/BIM/0.03137254901960784eps.npz'\n",
    "adv_path2 = 'adv samples/CIFAR/BIM/0.06274509803921569eps.npz'\n",
    "adv_path3 = 'adv samples/CIFAR/BIM/0.12549019607843137eps.npz'\n",
    "adv_path4 = 'adv samples/CIFAR/BIM/0.25098039215686274eps.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faea2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.765647,\n",
       " [0.014000000000000002,\n",
       "  0.05800000000000001,\n",
       "  0.106,\n",
       "  0.204,\n",
       "  0.342,\n",
       "  0.532,\n",
       "  0.708,\n",
       "  0.872,\n",
       "  0.968,\n",
       "  0.992,\n",
       "  1.0],\n",
       " [0.04,\n",
       "  0.146,\n",
       "  0.391,\n",
       "  0.613,\n",
       "  0.773,\n",
       "  0.8859999999999999,\n",
       "  0.95,\n",
       "  0.978,\n",
       "  0.9940000000000001,\n",
       "  0.9990000000000001,\n",
       "  1.0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc, tpr, fpr = return_auc(adv_path1, normal_model)\n",
    "auc, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fbb9981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.8440654999999999, [0.014000000000000002, 0.04100000000000001, 0.102, 0.207, 0.32899999999999996, 0.517, 0.709, 0.885, 0.965, 0.996, 1.0], [0.103, 0.255, 0.543, 0.764, 0.879, 0.951, 0.983, 0.992, 0.998, 1.0, 1.0])\n",
      "----\n",
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.9255890000000001, [0.019, 0.061, 0.111, 0.22699999999999998, 0.353, 0.525, 0.7070000000000001, 0.878, 0.973, 0.997, 1.0], [0.218, 0.518, 0.875, 0.9620000000000001, 0.99, 0.998, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "----\n",
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n",
      "(0.975921, [0.017, 0.039, 0.095, 0.18600000000000003, 0.327, 0.519, 0.687, 0.862, 0.958, 0.9940000000000001, 1.0], [0.61, 0.939, 0.996, 0.9990000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "print(return_auc(adv_path2, normal_model))\n",
    "print('----')\n",
    "print(return_auc(adv_path3, normal_model))\n",
    "print('----')\n",
    "print(return_auc(adv_path4, normal_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bedf4",
   "metadata": {},
   "source": [
    "# CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67abd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_path1 = 'adv samples/CIFAR/CW/cwCIFAR.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fd8e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "Computing metrics for {} for adv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9758230000000001,\n",
       " [0.019,\n",
       "  0.057,\n",
       "  0.106,\n",
       "  0.22400000000000003,\n",
       "  0.332,\n",
       "  0.527,\n",
       "  0.699,\n",
       "  0.865,\n",
       "  0.961,\n",
       "  0.9990000000000001,\n",
       "  1.0],\n",
       " [0.806, 0.976, 0.996, 0.9990000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc, tpr, fpr = return_auc(adv_path1, normal_model)\n",
    "auc, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ac8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_detection",
   "language": "python",
   "name": "adv_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
