{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8df539ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from captum.attr import *\n",
    "import quantus\n",
    "import gc\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59cf4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d0a564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4d9fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    ''' A basic neural network model '''\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()         #python2 : super(MLP, self).__init__()\n",
    "        #defining the network's operations\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1], output_size)\n",
    "\n",
    "    def forward(self, x, softmax=False): \n",
    "        a = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x.float())))))\n",
    "        if softmax:\n",
    "            y_pred = F.softmax(a, dim=1)\n",
    "        else:\n",
    "            y_pred = a\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6b15766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=121, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=121\n",
    "hidden_size=[256,256]\n",
    "output_size=2\n",
    "model = Network(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(\"./model.pytorch\"))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12cad7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x_batch, y_batch, spread):\n",
    "    new_x_batch = []\n",
    "    for x in x_batch:\n",
    "        x = x.data.cpu().numpy()\n",
    "        stdev = spread * (np.max(x)-np.min(x))\n",
    "        #print(stdev)\n",
    "        noise = np.random.normal(0, stdev, x.shape).astype(np.float32)\n",
    "        x_plus_noise = x + noise\n",
    "        x_plus_noise = np.clip(x_plus_noise, 0, 1)\n",
    "        x_plus_noise = torch.from_numpy(x_plus_noise).cpu()\n",
    "        new_x_batch.append(x_plus_noise)\n",
    "    new_batch = torch.stack(new_x_batch).to(device)\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedee644",
   "metadata": {},
   "source": [
    "# On FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "672fbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_benign(train_loader, normal_model): \n",
    "    \n",
    "    print(\"Computing metrics for {} for benign\")\n",
    "\n",
    "    \n",
    "    #attribution robustness\n",
    "    attribution_gaussian1 = []\n",
    "    attribution_gaussian2 = []\n",
    "    attribution_gaussian3 = []\n",
    "    attribution_gaussian4 = []\n",
    "    attribution_gaussian5 = []\n",
    "    \n",
    "    #logit robustness\n",
    "    logit_gaussian1 = []\n",
    "    logit_gaussian2 = []\n",
    "    logit_gaussian3 = []\n",
    "    logit_gaussian4 = []\n",
    "    logit_gaussian5 = []\n",
    "    \n",
    "    for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        images_adv, y_pred_adv = x_batch.to(device), y_batch.to(device)\n",
    "        x_logits = normal_model(images_adv)\n",
    "        \n",
    "        a_batch = quantus.explain(\n",
    "            model=normal_model, inputs=images_adv, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        gaussian_noisy_images_1 = make_noise(images_adv, y_pred_adv, spread = 0.0005)\n",
    "        gaussian_logits_1 = normal_model(gaussian_noisy_images_1)\n",
    "        \n",
    "        gaussian_noisy_images_2 = make_noise(images_adv, y_pred_adv, spread = 0.001)\n",
    "        gaussian_logits_2 = normal_model(gaussian_noisy_images_2)\n",
    "        \n",
    "        gaussian_noisy_images_3 = make_noise(images_adv, y_pred_adv, spread = 0.005)\n",
    "        gaussian_logits_3 = normal_model(gaussian_noisy_images_3)\n",
    "        \n",
    "        gaussian_noisy_images_4 = make_noise(images_adv, y_pred_adv, spread = 0.01)\n",
    "        gaussian_logits_4 = normal_model(gaussian_noisy_images_4)\n",
    "        \n",
    "        gaussian_noisy_images_5 = make_noise(images_adv, y_pred_adv, spread = 0.1)\n",
    "        gaussian_logits_5 = normal_model(gaussian_noisy_images_5)\n",
    "        \n",
    "        \n",
    "        diff1 = torch.norm(x_logits-gaussian_logits_1,p=1, dim=1) \n",
    "        diff2 = torch.norm(x_logits-gaussian_logits_2,p=1, dim=1) \n",
    "        diff3 = torch.norm(x_logits-gaussian_logits_3,p=1, dim=1) \n",
    "        diff4 = torch.norm(x_logits-gaussian_logits_4,p=1, dim=1) \n",
    "        diff5 = torch.norm(x_logits-gaussian_logits_5,p=1, dim=1) \n",
    "        \n",
    "        logit_gaussian1.extend(diff1.detach().cpu().numpy())\n",
    "        logit_gaussian2.extend(diff2.detach().cpu().numpy())\n",
    "        logit_gaussian3.extend(diff3.detach().cpu().numpy())\n",
    "        logit_gaussian4.extend(diff4.detach().cpu().numpy())\n",
    "        logit_gaussian5.extend(diff5.detach().cpu().numpy())\n",
    "        \n",
    "        a_batch_gaussian1 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_1, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian2 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_2, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian3 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_3, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian4 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_4, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian5 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_5, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        \n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian1):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian1.append(c)\n",
    "            \n",
    "        for a, b in zip(a_batch, a_batch_gaussian2):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian2.append(c)\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian3):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian3.append(c)\n",
    "            \n",
    "        for a, b in zip(a_batch, a_batch_gaussian4):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian4.append(c)\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian5):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian5.append(c)\n",
    "        \n",
    "       \n",
    "        if step%20==0:\n",
    "            print(step)\n",
    "        if step > 50:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame([\n",
    "            \n",
    "            attribution_gaussian1,\n",
    "            attribution_gaussian2,\n",
    "            attribution_gaussian3,\n",
    "        attribution_gaussian4,\n",
    "        attribution_gaussian5,\n",
    "            logit_gaussian1,\n",
    "            logit_gaussian2,\n",
    "            logit_gaussian3,\n",
    "    logit_gaussian4,\n",
    "    logit_gaussian5], index = [\n",
    "            \"Gaussian1 attribution\", \n",
    "            \"Gaussian2 attribution\", \n",
    "            \"Gaussian3 attribution\",\n",
    "        \"Gaussian4 attribution\",\n",
    "        \"Gaussian5 attribution\",\n",
    "            \"Gaussian1 logit robusntess\",\n",
    "            \"Gaussian2 logit robusntess\",\n",
    "            \"Gaussian3 logit robusntess\",\n",
    "        \"Gaussian4 logit robusntess\",\n",
    "        \"Gaussian5 logit robusntess\"\n",
    "                    ])\n",
    "            \n",
    "    path = \"kdd_Benign_eval.csv\"\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac247bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data = self.data.drop(self.data.columns[0], axis=1)  # Remove unnecessary index column\n",
    "        self.features = self.data.iloc[:, :-1].values  # Features (all columns except the last)\n",
    "        self.labels = self.data.iloc[:, -1].values     # Labels (last column)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Load CSV file and split into train and test sets\n",
    "dataset = CustomDataset('clean_examples.csv')\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d65a0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2455a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_adv(train_loader, normal_model): \n",
    "    \n",
    "    print(\"Computing metrics for {} for benign\")\n",
    "\n",
    "    \n",
    "    #attribution robustness\n",
    "    attribution_gaussian1 = []\n",
    "    attribution_gaussian2 = []\n",
    "    attribution_gaussian3 = []\n",
    "    attribution_gaussian4 = []\n",
    "    attribution_gaussian5 = []\n",
    "    \n",
    "    #logit robustness\n",
    "    logit_gaussian1 = []\n",
    "    logit_gaussian2 = []\n",
    "    logit_gaussian3 = []\n",
    "    logit_gaussian4 = []\n",
    "    logit_gaussian5 = []\n",
    "    \n",
    "    for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        images_adv, y_pred_adv = x_batch.to(device), y_batch.to(device)\n",
    "        x_logits = normal_model(images_adv)\n",
    "        \n",
    "        a_batch = quantus.explain(\n",
    "            model=normal_model, inputs=images_adv, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        gaussian_noisy_images_1 = make_noise(images_adv, y_pred_adv, spread = 0.0005)\n",
    "        gaussian_logits_1 = normal_model(gaussian_noisy_images_1)\n",
    "        \n",
    "        gaussian_noisy_images_2 = make_noise(images_adv, y_pred_adv, spread = 0.001)\n",
    "        gaussian_logits_2 = normal_model(gaussian_noisy_images_2)\n",
    "        \n",
    "        gaussian_noisy_images_3 = make_noise(images_adv, y_pred_adv, spread = 0.005)\n",
    "        gaussian_logits_3 = normal_model(gaussian_noisy_images_3)\n",
    "        \n",
    "        gaussian_noisy_images_4 = make_noise(images_adv, y_pred_adv, spread = 0.01)\n",
    "        gaussian_logits_4 = normal_model(gaussian_noisy_images_4)\n",
    "        \n",
    "        gaussian_noisy_images_5 = make_noise(images_adv, y_pred_adv, spread = 0.1)\n",
    "        gaussian_logits_5 = normal_model(gaussian_noisy_images_5)\n",
    "        \n",
    "        \n",
    "        diff1 = torch.norm(x_logits-gaussian_logits_1,p=1, dim=1) \n",
    "        diff2 = torch.norm(x_logits-gaussian_logits_2,p=1, dim=1) \n",
    "        diff3 = torch.norm(x_logits-gaussian_logits_3,p=1, dim=1) \n",
    "        diff4 = torch.norm(x_logits-gaussian_logits_4,p=1, dim=1) \n",
    "        diff5 = torch.norm(x_logits-gaussian_logits_5,p=1, dim=1) \n",
    "        \n",
    "        logit_gaussian1.extend(diff1.detach().cpu().numpy())\n",
    "        logit_gaussian2.extend(diff2.detach().cpu().numpy())\n",
    "        logit_gaussian3.extend(diff3.detach().cpu().numpy())\n",
    "        logit_gaussian4.extend(diff4.detach().cpu().numpy())\n",
    "        logit_gaussian5.extend(diff5.detach().cpu().numpy())\n",
    "        \n",
    "        a_batch_gaussian1 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_1, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian2 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_2, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian3 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_3, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian4 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_4, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        a_batch_gaussian5 = quantus.explain(\n",
    "        model=normal_model, inputs=gaussian_noisy_images_5, targets=y_pred_adv, **{\"method:\": \"IntegratedGradient\", \"device\": device})\n",
    "        \n",
    "        \n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian1):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian1.append(c)\n",
    "            \n",
    "        for a, b in zip(a_batch, a_batch_gaussian2):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian2.append(c)\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian3):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian3.append(c)\n",
    "            \n",
    "        for a, b in zip(a_batch, a_batch_gaussian4):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian4.append(c)\n",
    "        \n",
    "        for a, b in zip(a_batch, a_batch_gaussian5):\n",
    "            c = np.linalg.norm(a.flatten()-b.flatten(),ord=1 )\n",
    "            attribution_gaussian5.append(c)\n",
    "        \n",
    "       \n",
    "        if step%20==0:\n",
    "            print(step)\n",
    "        if step > 50:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame([\n",
    "            \n",
    "            attribution_gaussian1,\n",
    "            attribution_gaussian2,\n",
    "            attribution_gaussian3,\n",
    "        attribution_gaussian4,\n",
    "        attribution_gaussian5,\n",
    "            logit_gaussian1,\n",
    "            logit_gaussian2,\n",
    "            logit_gaussian3,\n",
    "    logit_gaussian4,\n",
    "    logit_gaussian5], index = [\n",
    "            \"Gaussian1 attribution\", \n",
    "            \"Gaussian2 attribution\", \n",
    "            \"Gaussian3 attribution\",\n",
    "        \"Gaussian4 attribution\",\n",
    "        \"Gaussian5 attribution\",\n",
    "            \"Gaussian1 logit robusntess\",\n",
    "            \"Gaussian2 logit robusntess\",\n",
    "            \"Gaussian3 logit robusntess\",\n",
    "        \"Gaussian4 logit robusntess\",\n",
    "        \"Gaussian5 logit robusntess\"\n",
    "                    ])\n",
    "            \n",
    "    path = \"kdd_Adv_eval.csv\"\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58c8bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[0.0015,0.0015,0.0015, 0.0015,0.0015,0.0015,0.0015,0.0015]\n",
    "l=[1.07,1.00,0.8, 0.7, 0.6, 0.3,0.2,0.1]\n",
    "m=[0.0,0.0,0.0, 0,0,0,0,0]\n",
    "n=[100,40,25, 20, 18, 13, 7, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6b4272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TPR(adv1, a, b, adv2, c, d): \n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP = 0 \n",
    "    TP=0\n",
    "    \n",
    "    for value1, value2 in zip(adv1, adv2): \n",
    "        if value1<a or value1>b:\n",
    "            TP += 1\n",
    "        else:\n",
    "            if value2<c or value2>d:\n",
    "                TP+=1\n",
    "            else: \n",
    "                FN+=1\n",
    "    \n",
    "    return (TP/(TP+FN))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab7ae625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FPR(ap2a, k, l, ap2b, m, n): \n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0 \n",
    "    TP=0\n",
    "    \n",
    "    for value6, value7 in zip(ap2a,ap2b):\n",
    "        if value6<k or value6>l:\n",
    "            FP +=1\n",
    "        else:\n",
    "            if value7<m or value7>n:\n",
    "                FP +=1\n",
    "\n",
    "    return (FP/(len(ap2a)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "35f677f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ab65b89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a08e24b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "30afc002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e433b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.664674225876999, 0.6862745350967275, 0.6680753242019222, 0.670078893826213]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "457f6ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6722757447504655 0.008309917374894303\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc), np.std(sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a79e71",
   "metadata": {},
   "source": [
    "# PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3fc89f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "094e177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_PGD1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cbe53ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_PGD2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f0850ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_PGD3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b2c2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_PGD4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ca3cde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6835132435914226,\n",
       " 0.6730930880983335,\n",
       " 0.6850003088251608,\n",
       " 0.6785664260157349]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "162fd05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.680043266632663 0.004666343516592785\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc), np.std(sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f7a7a",
   "metadata": {},
   "source": [
    "# BIM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e3404f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2b0195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "74fc97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "efbfe141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "758fa0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and split into train and test sets\n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b1cb8b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6569768915313463, 0.6741421831040313, 0.669853231411419, 0.6666928194280315]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "761e240b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6669162813687071, 0.006318242180779156)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sc), np.std(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b31ca",
   "metadata": {},
   "source": [
    "# CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "80af0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "sc = [] \n",
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW21.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "060b8bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW22.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "81090ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW23.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "810702a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "Computing metrics for {} for benign\n",
      "0\n",
      "20\n",
      "40\n",
      "2567 2567\n",
      "2567 2567\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_benign(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW24.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "compute_metrics_adv(test_loader_adv, model)\n",
    "\n",
    "df_ben = pd.read_csv(\"kdd_Benign_eval.csv\")\n",
    "attr_gaussian3 = df_ben.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3 = df_ben.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3), len(logit_gaussian3))\n",
    "fpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    FPR = compute_FPR(logit_gaussian3, t1,t2, attr_gaussian3,t3,t4)\n",
    "    fpr_results.append(FPR/100)\n",
    "    \n",
    "df_adv= pd.read_csv(\"kdd_Adv_eval.csv\")\n",
    "attr_gaussian3_eps1 = df_adv.iloc[0].values.flatten().tolist()[1:]\n",
    "logit_gaussian3_eps1 = df_adv.iloc[5].values.flatten().tolist()[1:]\n",
    "print(len(attr_gaussian3_eps1), len(logit_gaussian3_eps1)) \n",
    "tpr_results =[]\n",
    "for t1,t2,t3,t4 in zip(k,l,m,n):\n",
    "    TPR = compute_TPR(logit_gaussian3_eps1, t1,t2, attr_gaussian3_eps1,t3,t4)\n",
    "    tpr_results.append(TPR/100)\n",
    "    \n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a9312faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7182395326860701, 0.007571592645126742)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sc), np.std(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8fc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_detection",
   "language": "python",
   "name": "adv_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
