{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "03e0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from captum.attr import *\n",
    "import quantus\n",
    "import gc\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4552b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7d4ccf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "091ab08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    ''' A basic neural network model '''\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()         #python2 : super(MLP, self).__init__()\n",
    "        #defining the network's operations\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1], output_size)\n",
    "\n",
    "    def forward(self, x, softmax=False): \n",
    "        x = x.reshape(-1,83)\n",
    "        a = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x.float())))))\n",
    "        if softmax:\n",
    "            y_pred = F.softmax(a, dim=1)\n",
    "        else:\n",
    "            y_pred = a\n",
    "        #print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a81954ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=83, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 83\n",
    "hidden_size=[64,64]\n",
    "output_size=2\n",
    "model = Network(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(\"./model_bce_loss.pytorch\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b0a7e3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>115405386.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>34044.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.968750</td>\n",
       "      <td>...</td>\n",
       "      <td>39203.0</td>\n",
       "      <td>9.973434e+06</td>\n",
       "      <td>57446.079336</td>\n",
       "      <td>9997094.0</td>\n",
       "      <td>9800577.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>115405386.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>59062.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14766461.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>47056.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>4619105.0</td>\n",
       "      <td>1.000091e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000912.0</td>\n",
       "      <td>10000912.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>14766461.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5907768.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>5.838974e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5838974.0</td>\n",
       "      <td>5838974.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5907768.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395596</th>\n",
       "      <td>395596</td>\n",
       "      <td>17.0</td>\n",
       "      <td>32231.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395597</th>\n",
       "      <td>395597</td>\n",
       "      <td>6.0</td>\n",
       "      <td>115643087.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>4380.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.227273</td>\n",
       "      <td>...</td>\n",
       "      <td>23475.0</td>\n",
       "      <td>1.001795e+07</td>\n",
       "      <td>19260.895758</td>\n",
       "      <td>10024497.0</td>\n",
       "      <td>9960011.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>115643087.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395598</th>\n",
       "      <td>395598</td>\n",
       "      <td>17.0</td>\n",
       "      <td>122275.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395599</th>\n",
       "      <td>395599</td>\n",
       "      <td>6.0</td>\n",
       "      <td>115442585.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>7368.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.391304</td>\n",
       "      <td>...</td>\n",
       "      <td>23238.0</td>\n",
       "      <td>1.019040e+07</td>\n",
       "      <td>66002.373598</td>\n",
       "      <td>10216763.0</td>\n",
       "      <td>10003600.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>115442585.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395600</th>\n",
       "      <td>395600</td>\n",
       "      <td>6.0</td>\n",
       "      <td>117841438.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24378.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>5520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1160.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>86069.0</td>\n",
       "      <td>5.855763e+07</td>\n",
       "      <td>523806.318727</td>\n",
       "      <td>58928018.0</td>\n",
       "      <td>58187244.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117841438.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395601 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     0            1     2     3        4        5       6  \\\n",
       "0                0  17.0      23999.0   1.0   1.0     77.0    159.0    77.0   \n",
       "1                1   6.0  115405386.0  32.0  34.0    703.0  34044.0   337.0   \n",
       "2                2  17.0      59062.0   2.0   2.0     70.0    216.0    35.0   \n",
       "3                3   6.0   14766461.0  23.0  30.0    893.0  47056.0   517.0   \n",
       "4                4   6.0    5907768.0   4.0   2.0      0.0      0.0     0.0   \n",
       "...            ...   ...          ...   ...   ...      ...      ...     ...   \n",
       "395596      395596  17.0      32231.0   2.0   2.0     70.0    192.0    35.0   \n",
       "395597      395597   6.0  115643087.0  22.0  20.0    995.0   4380.0   613.0   \n",
       "395598      395598  17.0     122275.0   1.0   1.0     50.0    136.0    50.0   \n",
       "395599      395599   6.0  115442585.0  23.0  20.0    745.0   7368.0   391.0   \n",
       "395600      395600   6.0  117841438.0  21.0  29.0  24378.0   5650.0  5520.0   \n",
       "\n",
       "           7            8  ...         74            75             76  \\\n",
       "0       77.0    77.000000  ...        0.0  0.000000e+00       0.000000   \n",
       "1        0.0    21.968750  ...    39203.0  9.973434e+06   57446.079336   \n",
       "2       35.0    35.000000  ...        0.0  0.000000e+00       0.000000   \n",
       "3        0.0    38.826087  ...  4619105.0  1.000091e+07       0.000000   \n",
       "4        0.0     0.000000  ...    34442.0  5.838974e+06       0.000000   \n",
       "...      ...          ...  ...        ...           ...            ...   \n",
       "395596  35.0    35.000000  ...        0.0  0.000000e+00       0.000000   \n",
       "395597   0.0    45.227273  ...    23475.0  1.001795e+07   19260.895758   \n",
       "395598  50.0    50.000000  ...        0.0  0.000000e+00       0.000000   \n",
       "395599   0.0    32.391304  ...    23238.0  1.019040e+07   66002.373598   \n",
       "395600   0.0  1160.857143  ...    86069.0  5.855763e+07  523806.318727   \n",
       "\n",
       "                77          78   79   80           81   82   83  \n",
       "0              0.0         0.0 -1.0 -1.0          0.0 -1.0  0.0  \n",
       "1        9997094.0   9800577.0 -1.0 -1.0  115405386.0 -1.0  0.0  \n",
       "2              0.0         0.0 -1.0 -1.0          0.0 -1.0  0.0  \n",
       "3       10000912.0  10000912.0 -1.0 -1.0   14766461.0 -1.0  0.0  \n",
       "4        5838974.0   5838974.0 -1.0 -1.0    5907768.0 -1.0  0.0  \n",
       "...            ...         ...  ...  ...          ...  ...  ...  \n",
       "395596         0.0         0.0 -1.0 -1.0          0.0 -1.0  0.0  \n",
       "395597  10024497.0   9960011.0 -1.0 -1.0  115643087.0 -1.0  0.0  \n",
       "395598         0.0         0.0 -1.0 -1.0          0.0 -1.0  0.0  \n",
       "395599  10216763.0  10003600.0 -1.0 -1.0  115442585.0 -1.0  0.0  \n",
       "395600  58928018.0  58187244.0 -1.0 -1.0  117841438.0 -1.0  0.0  \n",
       "\n",
       "[395601 rows x 85 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_data_before_scaling_class_0.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a3a01",
   "metadata": {},
   "source": [
    "- True positive is when adversarial is identified as malware. \n",
    "- False positive is when benign is identified as malware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fc79a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TPR(adv, th): \n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP = 0 \n",
    "    TP=0\n",
    "    \n",
    "    for value in adv: \n",
    "        if value<th:\n",
    "            TP += 1\n",
    "        else: \n",
    "            FN += 1\n",
    "    return (TP/(TP+FN))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2af5b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FPR(ben, th): \n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP = 0 \n",
    "    TP=0\n",
    "    \n",
    "    for value in ben: \n",
    "        if value>th:\n",
    "            TN += 1\n",
    "        else: \n",
    "            FP += 1\n",
    "    return (FP/(FP+TN))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "08b3dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iqr(attr):\n",
    "    scores = []\n",
    "    for i in range(len(attr)):\n",
    "        a = attr[i].flatten()\n",
    "        score_75 = np.percentile(a, 75)\n",
    "        score_25 = np.percentile(a, 25)\n",
    "        score_qt = score_75 - score_25\n",
    "        scores.append(score_qt)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6267f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fs(model, x_batch, y_batch, train_fpr):\n",
    "    distances = get_attr(model, x_batch, y_batch)\n",
    "    selected_distance_idx = int(np.ceil(len(x_batch) * (1-train_fpr)))\n",
    "    threshold = sorted(distances)[selected_distance_idx-1]\n",
    "    threshold = threshold\n",
    "    return threshold\n",
    "\n",
    "def get_attr(model, x_batch, y_batch):\n",
    "    ig = IntegratedGradients(model)\n",
    "    a_batch_intgrad = ig.attribute(inputs=x_batch, target=y_batch).to('cpu') \n",
    "    iqr = compute_iqr(a_batch_intgrad)\n",
    "    return iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1ade8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on benign samples and save metrics\n",
    "def compute_benign_metrics(test_loader, model):\n",
    "    fs = []\n",
    "    i=0\n",
    "    for image, label in test_loader:\n",
    "        i += len(image)\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        d = get_attr(model, image, label)    \n",
    "        fs.extend(d)\n",
    "        if i>1000:\n",
    "            return fs \n",
    "    return fs\n",
    "\n",
    "\n",
    "#evaluate on adv samples and save metrics\n",
    "def compute_adv_metrics(test_loader, model):\n",
    "    fs = []\n",
    "    i=0\n",
    "    for image, label in test_loader: \n",
    "        i += len(image) \n",
    "        image, label = image.to(device), label.to(device)\n",
    "        d = get_attr(model, image, label)    \n",
    "        fs.extend(d)\n",
    "        if i>1000:\n",
    "            return fs \n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "952161e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data = self.data.drop(self.data.columns[0], axis=1)  # Remove unnecessary index column\n",
    "        self.features = self.data.iloc[:, :-1].values  # Features (all columns except the last)\n",
    "        self.labels = self.data.iloc[:, -1].values     # Labels (last column)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Load CSV file and split into train and test sets\n",
    "dataset = CustomDataset('clean_data_before_scaling_class_0.csv')\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "05e254a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for 0.01 FPR is 1039.0377512001608.\n",
      "Threshold for 0.05 FPR is 321.25987808878375.\n",
      "Threshold for 0.1 FPR is 195.2231011156793.\n"
     ]
    }
   ],
   "source": [
    "FPR = [0.01,0.05,0.1]\n",
    "final_th = []\n",
    "for fpr in FPR:\n",
    "    t=[]\n",
    "    for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        #print(x_batch.shape)\n",
    "        #print(y_batch)\n",
    "        threshold = train_fs(model, x_batch, y_batch, fpr)\n",
    "        t.append(threshold.item())\n",
    "        if step==50:\n",
    "            break\n",
    "    th = sum(t)/len(t)\n",
    "    print(\"Threshold for {} FPR is {}.\".format(fpr, th))\n",
    "    final_th.append(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "044e6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = [1000,800,600,400,200,100,50,20,10,5,2,1,0.5,0.005,0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a953d89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.990234375, 0.9873046875, 0.9794921875, 0.9609375, 0.8955078125, 0.8046875, 0.7001953125, 0.6240234375, 0.5732421875, 0.49609375, 0.361328125, 0.21484375, 0.203125, 0.1904296875, 0.1904296875]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.805, 0.435, 0.0, 0.0]\n",
      "0.76832275390625\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "print(fpr_results)\n",
    "print(tpr_results)\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4d488a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74116455078125\n",
      "0.6940429687499999\n",
      "---\n",
      "0.6641259765624999\n",
      "---\n",
      "0.61629638671875\n"
     ]
    }
   ],
   "source": [
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('---')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('---')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_FGSM5.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "990fff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69679052734375 0.05411052027830608\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc),np.std(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a8211d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75349609375\n",
      "---\n",
      "0.69554443359375\n",
      "---\n",
      "0.6350341796875001\n",
      "---\n",
      "0.6022778320312501\n",
      "---\n",
      "0.59085205078125\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_PGD1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('---')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_PGD2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('---')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_PGD3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('---')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_PGD4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('---')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_PGD5.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7f815be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65544091796875 0.0610533863126664\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc),np.std(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4c63f232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7648193359375\n",
      "----\n",
      "0.7648193359375\n",
      "----\n",
      "0.7648193359375\n",
      "----\n",
      "0.7648193359375\n",
      "----\n",
      "0.7648193359375\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_BIM5.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3975aa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7648193359375 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc),np.std(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "eaaf71f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75978515625\n",
      "----\n",
      "0.7584716796875\n",
      "----\n",
      "0.75853271484375\n",
      "----\n",
      "0.75749755859375\n",
      "----\n",
      "0.75670654296875\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW1.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "\n",
    "advdataset = CustomDataset('adversarial_examples_CW5.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "44dbd554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581987304687499 0.0010412549285955229\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc),np.std(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "66eebc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62978759765625\n",
      "----\n",
      "0.6295410156250001\n",
      "----\n",
      "0.62996337890625\n",
      "----\n",
      "0.6297607421875001\n",
      "----\n",
      "0.62917236328125\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_autoPGD.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_autoPGD2.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_autoPGD3.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_autoPGD4.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n",
    "print('----')\n",
    "\n",
    "dist_ben = compute_benign_metrics(test_loader, model)\n",
    "advdataset = CustomDataset('adversarial_examples_autoPGD5.csv')\n",
    "advtrain_data, advtest_data = train_test_split(advdataset, test_size=0.2, random_state=42)\n",
    "batch_size = 64\n",
    "train_loader_adv = DataLoader(advtrain_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_adv = DataLoader(advtest_data, batch_size=batch_size, shuffle=False)\n",
    "dist_adv = compute_adv_metrics(test_loader_adv, model)\n",
    "fpr_results = []\n",
    "for th in threshold:\n",
    "    FPR = compute_FPR(dist_ben, th)\n",
    "    fpr_results.append(FPR/100)\n",
    "tpr_results = []\n",
    "for th in threshold:\n",
    "    TPR = compute_TPR(dist_adv, th)\n",
    "    tpr_results.append(TPR/100)\n",
    "\n",
    "\n",
    "score = sklearn.metrics.auc(fpr_results, tpr_results)\n",
    "print(score)\n",
    "sc.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2ac0380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6296450195312501 0.00027176964132432247\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sc),np.std(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29248af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
